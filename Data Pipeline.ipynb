{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning-python in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from lightning-python)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->lightning-python)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->lightning-python)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->lightning-python)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->lightning-python)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->lightning-python)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->lightning-python)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->lightning-python)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->lightning-python)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->lightning-python)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->lightning-python)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->lightning-python)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->lightning-python)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->lightning-python)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install lightning-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql.functions import col, split, ltrim, substring\n",
    "import pyspark.sql as SQL\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "import calendar\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Jan-01\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Jan-01').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and decompress data into your Jupyter environment; abreviated jan 2017 data\n",
    "jan_2017 = spark.read.format(\"csv\").load('yellow_tripdata_half.csv', header = True).cache()\n",
    "#jan_2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get two dataframes to merge on, or else get cartesian product error\n",
    "taxi_zone = spark.read.format(\"csv\").load('taxi+_zone_lookup.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging to get destination information\n",
    "jan_2017 = jan_2017.join(taxi_zone, jan_2017.PULocationID == taxi_zone.LocationID, \"left_outer\"). \\\n",
    "                withColumnRenamed(\"Borough\", \"PUBorough\").withColumnRenamed(\"Zone\", \"PUZone\").withColumnRenamed(\"service_zone\", \"PUServiceZone\").\\\n",
    "                withColumnRenamed(\"neighborhood\", \"PUneighbor\").cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make unique ID\n",
    "jan_2017 = jan_2017.withColumn(\"uniqueIdColumn\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.drop(\"LocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding if pickup is an aiport\n",
    "jan_2017 = jan_2017.withColumn(\"AirportPU\", \\\n",
    "                               F.when((jan_2017[\"PULocationID\"] == '138' ) | \\\n",
    "                                      (jan_2017[\"PULocationID\"] == '132') |\\\n",
    "                                      (jan_2017[\"PULocationID\"] == '1'),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning data\n",
    "jan_2017 = jan_2017.where((jan_2017['PUBorough'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up time and date\n",
    "split_pickup_col = split(jan_2017['tpep_pickup_datetime'], ' ')\n",
    "jan_2017 = jan_2017.withColumn(\"PUDate\", split_pickup_col.getItem(0).cast(DateType()))\n",
    "jan_2017 = jan_2017.withColumn(\"PUTime\", split_pickup_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting time into hour and minute; will round minute to nearest 5 minutes\n",
    "split_PUTime = split(jan_2017['PUTime'], ':')\n",
    "jan_2017 = jan_2017.withColumn(\"PUHour\", split_PUTime.getItem(0).cast(IntegerType()))\n",
    "jan_2017 = jan_2017.withColumn(\"PUMinute\", split_PUTime.getItem(1).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rush hour\n",
    "jan_2017 = jan_2017.withColumn(\"MorningRushHour\", \\\n",
    "                               F.when((jan_2017[\"PUHour\"] >= 6 ) & \\\n",
    "                                      (jan_2017[\"PUHour\"] < 9),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.withColumn(\"EveningRushHour\", \\\n",
    "                               F.when((jan_2017[\"PUHour\"] >= 17 ) & \\\n",
    "                                      (jan_2017[\"PUHour\"] < 21),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.withColumn(\"PUDay\", dayofyear(jan_2017.PUDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding down mintue to closest 5 minute mark (computationally easier)\n",
    "#jan_2017 = jan_2017.withColumn(\"DOMinute\", (jan_2017.DOMinute - jan_2017.DOMinute%5))\n",
    "jan_2017 = jan_2017.withColumn(\"PUMinute\", (jan_2017.PUMinute - jan_2017.PUMinute%5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOW gives you 1 (Monday) - 7 (Sunday)\n",
    "jan_2017 = jan_2017.withColumn(\"PU_DOW\",  date_format(jan_2017.PUDate, 'u').cast(ShortType()))\n",
    "#jan_2017 = jan_2017.withColumn(\"DO_DOW\",  date_format(jan_2017.DODate, 'u').cast(ShortType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding if destination is a weekend\n",
    "jan_2017 = jan_2017.withColumn(\"Weekend\", \\\n",
    "                               F.when((jan_2017[\"PU_DOW\"] == 7) | \\\n",
    "                                      (jan_2017[\"PU_DOW\"] == 6),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.withColumn(\"WorkingHour\", \\\n",
    "                               F.when((((jan_2017[\"PUHour\"] >= 9 ) & (jan_2017[\"PUHour\"] < 17))\\\n",
    "                                       & (jan_2017[\"Weekend\"] == 0)) ,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting data types to primitives\n",
    "\n",
    "#1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.\n",
    "jan_2017 = jan_2017.withColumn(\"VendorID\", jan_2017[\"VendorID\"].cast(ShortType()))\n",
    "\n",
    "jan_2017 = jan_2017.withColumn(\"passenger_count\", jan_2017[\"passenger_count\"].cast(ShortType()))\n",
    "\n",
    "#in miles\n",
    "jan_2017 = jan_2017.withColumn(\"trip_distance\", jan_2017[\"trip_distance\"].cast(FloatType()))\n",
    "\n",
    "#1= Credit card\n",
    "#2= Cash\n",
    "#3= No charge\n",
    "#4= Dispute\n",
    "#5= Unknown\n",
    "#6= Voided trip\n",
    "jan_2017 = jan_2017.withColumn(\"payment_type\", jan_2017[\"payment_type\"].cast(ShortType()))\n",
    "jan_2017 = jan_2017.withColumn(\"fare_amount\", jan_2017[\"fare_amount\"].cast(FloatType()))\n",
    "\n",
    "#0.50 and $1 rush hour and overnight charges.\n",
    "jan_2017 = jan_2017.withColumn(\"extra\", jan_2017[\"extra\"].cast(FloatType()))\n",
    "#.50, automatic MTA charge\n",
    "jan_2017 = jan_2017.withColumn(\"mta_tax\", jan_2017[\"mta_tax\"].cast(FloatType()))\n",
    "\n",
    "\n",
    "jan_2017 = jan_2017.withColumn(\"tip_amount\", jan_2017[\"tip_amount\"].cast(FloatType()))\n",
    "jan_2017 = jan_2017.withColumn(\"tolls_amount\", jan_2017[\"tolls_amount\"].cast(FloatType()))\n",
    "jan_2017 = jan_2017.withColumn(\"improvement_surcharge\", jan_2017[\"improvement_surcharge\"].cast(FloatType()))\n",
    "jan_2017 = jan_2017.withColumn(\"total_amount\", jan_2017[\"total_amount\"].cast(FloatType()))\n",
    "\n",
    "\n",
    "jan_2017 = jan_2017.withColumn(\"RateCodeID\", jan_2017[\"RateCodeID\"].cast(ShortType()))\n",
    "#1= Standard rate\n",
    "#2=JFK -> $52 flat fare\n",
    "#3=Newark\n",
    "#4=Nassau or Westchester\n",
    "#5=Negotiated fare\n",
    "#6=Group ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic fare cleaning, ensure that all values are above zero\n",
    "jan_2017 = jan_2017.filter(jan_2017.tip_amount >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic fare cleaning, ensure that all values are above zero\n",
    "jan_2017 = jan_2017.filter(jan_2017.tolls_amount >= 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic fare cleaning, ensure that all values are above zero\n",
    "jan_2017 = jan_2017.filter(jan_2017.total_amount >= 3.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic fare cleaning, ensure that all values are above zero\n",
    "jan_2017 = jan_2017.filter(jan_2017.extra >= 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum fare amounts according to NYC Taxi data standards\n",
    "jan_2017 = jan_2017.filter((jan_2017.fare_amount >= 2.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum fare amounts according to NYC Taxi data standards\n",
    "jan_2017 = jan_2017.filter(jan_2017.improvement_surcharge >= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum fare amounts according to NYC Taxi data standards\n",
    "jan_2017 = jan_2017.filter(jan_2017.mta_tax >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.withColumn(\"PLocationID\", jan_2017.PULocationID.cast(IntegerType())).drop(\"PULocationID\")\n",
    "jan_2017 = jan_2017.withColumn(\"DLocationID\", jan_2017.DOLocationID.cast(IntegerType())).drop(\"DOLocationID\")\n",
    "#sampe.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jan_2017.printSchema()\n",
    "jan_2017 = jan_2017.drop('tpep_pickup_datetime')\n",
    "jan_2017 = jan_2017.drop('tpep_dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.drop('payment_type')\n",
    "jan_2017 = jan_2017.drop('fare_amount')\n",
    "jan_2017 = jan_2017.drop('extra')\n",
    "jan_2017 = jan_2017.drop('mta_tax')\n",
    "jan_2017 = jan_2017.drop('tip_amount')\n",
    "jan_2017 = jan_2017.drop('tolls_amount')\n",
    "jan_2017 = jan_2017.drop('improvement_surcharge')\n",
    "jan_2017 = jan_2017.drop('total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.drop('trip_distance')\n",
    "jan_2017 = jan_2017.drop('store_and_fwd_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weather data for merging\n",
    "weather_data = spark.read.load('weather.txt', format=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.createOrReplaceTempView('weather_data_sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = spark.sql('SELECT CAST(split(value, \",\")[0] as string) AS date, '\\\n",
    "                        'CAST(split(value, \",\")[1] as string) as time, '\\\n",
    "                        'CAST(split(value, \",\")[2] as float) as temp, '\\\n",
    "                        'CAST(split(value, \",\")[3] as float) as windchill, '\\\n",
    "                        'CAST(split(value, \",\")[4] as float) as dewpoint, '\\\n",
    "                        'CAST(split(value, \",\")[5] as float) as humidity, '\\\n",
    "                        'CAST(split(value, \",\")[6] as float) as pressure, '\\\n",
    "                        'CAST(split(value, \",\")[7] as float) as visibility, '\\\n",
    "                        'CAST(split(value, \",\")[8] as string) as windDir, '\\\n",
    "                        'CAST(split(value, \",\")[9] as float) as windSpeed, '\\\n",
    "                        'CAST(split(value, \",\")[10] as float) as gustSpeed, '\\\n",
    "                        'CAST(split(value, \",\")[11] as float) as Precip, '\\\n",
    "                        'CAST(split(value, \",\")[12] as string) as Events, '\\\n",
    "                        'CAST(split(value, \",\")[13] as string) as Conditions '\\\n",
    "                         'FROM weather_data_sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast date to date type\n",
    "weather_data = weather_data.withColumn(\"date\", weather_data.date.cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def period(x):\n",
    "    return split(split(x, ':')[1], \" \")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toHour(x):\n",
    "    first_split = split(x, ':')\n",
    "    retval = first_split[0].cast(IntegerType()) % 12\n",
    "    return retval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get am or pm\n",
    "weather_data = weather_data.withColumn(\"period\", period(\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make hour military time\n",
    "weather_data = weather_data.withColumn(\"hour\", when(weather_data.period == 'PM', toHour(\"time\") + 12).otherwise(toHour(\"time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fill any nulls\n",
    "weather_data = weather_data.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make temporary views for joining\n",
    "weather_data.createOrReplaceTempView('weather_data_sdf')\n",
    "\n",
    "weather_data_pu = spark.sql('SELECT date AS PUTempdate, '\\\n",
    "                            'time as PUTemptime, ' \\\n",
    "                            'temp as PUtemp, '\\\n",
    "                            'windchill as PUwindchill, '\\\n",
    "                            'dewpoint as PUdewpoint, '\\\n",
    "                            'pressure as PUpressure, '\\\n",
    "                            'visibility as PUvisibility, '\\\n",
    "                            'windDir as PUwindDir, '\\\n",
    "                            'gustSpeed as PUgustSpeed, '\\\n",
    "                            'Precip as PUPrecip, '\\\n",
    "                            'Events as PUEvents, '\\\n",
    "                            'Conditions as PUConditions, '\\\n",
    "                            'period as PUperiod, '\\\n",
    "                            'hour as PUTemphour '\\\n",
    "                            'FROM weather_data_sdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.join(weather_data_pu, (jan_2017.PUDate == weather_data_pu.PUTempdate) & \\\n",
    "                         (jan_2017.PUHour == weather_data_pu.PUTemphour), \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.dropDuplicates(['uniqueIdColumn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra, payment type, fare amount, mta_tax, tip_amount, tollsamount, total_amount, improvement surcharge\n",
    "\n",
    "# Categorical Features\n",
    "# RateCodeID\n",
    "# store_and_fwd_flag\n",
    "# PULocationID\n",
    "# DOLocationID\n",
    "# LocationID (1 to 256)\n",
    "# PUBorough (comes from taxi+_lookup_zone)\n",
    "# PUZone (Name for Location ID)\n",
    "# PUServiceZone (Categorical)\n",
    "# PUNeighbor (Demographics Neighborhood)\n",
    "# PUDay (1-365)\n",
    "# PU_DOW (Day of week)\n",
    "# PUEvents\n",
    "# PUConditions\n",
    "# PUPeriod (AM or PM)\n",
    "\n",
    "jan_2017 = jan_2017.drop('PUDate')\n",
    "jan_2017 = jan_2017.drop('PUTempdate')\n",
    "jan_2017 = jan_2017.drop('PUTime')\n",
    "jan_2017 = jan_2017.drop('PUTempdate')\n",
    "jan_2017 = jan_2017.drop('PUTemptime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.drop('PUTemphour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUdemographics = spark.read.format(\"csv\").load('demographics.csv', header = True).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUnames = PUdemographics.schema.names\n",
    "i = 0\n",
    "for name in PUnames:\n",
    "    if (i != 0):\n",
    "        PUdemographics = PUdemographics.withColumn(\"PU\" + name, col(name).cast(FloatType())).drop(name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUdemographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2017 = jan_2017.join(PUdemographics, jan_2017.PUneighbor == PUdemographics.neighborhood, \"left_outer\")\n",
    "jan_2017 = jan_2017.dropDuplicates(['uniqueIdColumn'])\n",
    "jan_2017 = jan_2017.drop('neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(input_sdf, col_name):\n",
    "    if (col_name == \"PUZone\"):\n",
    "        return input_sdf\n",
    "    else:\n",
    "        i = 0\n",
    "        col_vals = input_sdf.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "        for val in col_vals:\n",
    "            i += 1\n",
    "            input_sdf = input_sdf.withColumn(\"{0}_is_{1}\".format(col_name, val), \\\n",
    "                                           F.when(input_sdf[col_name] == val, 1).otherwise(0))\n",
    "        return input_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: short (nullable = true)\n",
      " |-- passenger_count: short (nullable = true)\n",
      " |-- RateCodeID: short (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUServiceZone: string (nullable = true)\n",
      " |-- PUneighbor: string (nullable = true)\n",
      " |-- uniqueIdColumn: long (nullable = false)\n",
      " |-- AirportPU: integer (nullable = false)\n",
      " |-- PUHour: integer (nullable = true)\n",
      " |-- PUMinute: integer (nullable = true)\n",
      " |-- MorningRushHour: integer (nullable = false)\n",
      " |-- EveningRushHour: integer (nullable = false)\n",
      " |-- PUDay: integer (nullable = true)\n",
      " |-- PU_DOW: short (nullable = true)\n",
      " |-- Weekend: integer (nullable = false)\n",
      " |-- WorkingHour: integer (nullable = false)\n",
      " |-- PLocationID: integer (nullable = true)\n",
      " |-- DLocationID: integer (nullable = true)\n",
      " |-- PUtemp: float (nullable = true)\n",
      " |-- PUwindchill: float (nullable = true)\n",
      " |-- PUdewpoint: float (nullable = true)\n",
      " |-- PUpressure: float (nullable = true)\n",
      " |-- PUvisibility: float (nullable = true)\n",
      " |-- PUwindDir: string (nullable = true)\n",
      " |-- PUgustSpeed: float (nullable = true)\n",
      " |-- PUPrecip: float (nullable = true)\n",
      " |-- PUEvents: string (nullable = true)\n",
      " |-- PUConditions: string (nullable = true)\n",
      " |-- PUperiod: string (nullable = true)\n",
      " |-- PUalone_hhld: float (nullable = true)\n",
      " |-- PUbachelor_higher: float (nullable = true)\n",
      " |-- PUbornstate: float (nullable = true)\n",
      " |-- PUcarfree: float (nullable = true)\n",
      " |-- PUcommutetime: float (nullable = true)\n",
      " |-- PUdisabled: float (nullable = true)\n",
      " |-- PUdisconnected: float (nullable = true)\n",
      " |-- PUforeign: float (nullable = true)\n",
      " |-- PUgross_rent_adj: float (nullable = true)\n",
      " |-- PUhhu18: float (nullable = true)\n",
      " |-- PUhomeownership: float (nullable = true)\n",
      " |-- PUhousing_units: float (nullable = true)\n",
      " |-- PUincome_diversity_ratio: float (nullable = true)\n",
      " |-- PUlaborforcerate: float (nullable = true)\n",
      " |-- PUmedhhincome_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_own_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_rent_adj: float (nullable = true)\n",
      " |-- PUnohsdiploma: float (nullable = true)\n",
      " |-- PUpark_share: float (nullable = true)\n",
      " |-- PUpasian: float (nullable = true)\n",
      " |-- PUpblack: float (nullable = true)\n",
      " |-- PUphisp: float (nullable = true)\n",
      " |-- PUpop65: float (nullable = true)\n",
      " |-- PUpopulation: float (nullable = true)\n",
      " |-- PUpopulation_density: float (nullable = true)\n",
      " |-- PUpov65older: float (nullable = true)\n",
      " |-- PUpovrate: float (nullable = true)\n",
      " |-- PUpovunder18: float (nullable = true)\n",
      " |-- PUprof_pct_ela: float (nullable = true)\n",
      " |-- PUprof_pct_math: float (nullable = true)\n",
      " |-- PUprop_rt: float (nullable = true)\n",
      " |-- PUpwhite: float (nullable = true)\n",
      " |-- PUrdindex: float (nullable = true)\n",
      " |-- PUrent_pct_nycha: float (nullable = true)\n",
      " |-- PUrentvacrate: float (nullable = true)\n",
      " |-- PUserious_viol_rate: float (nullable = true)\n",
      " |-- PUsevcrowd: float (nullable = true)\n",
      " |-- PUsubway_share: float (nullable = true)\n",
      " |-- PUtot_rt: float (nullable = true)\n",
      " |-- PUtotal_viol_rate: float (nullable = true)\n",
      " |-- PUunemprate: float (nullable = true)\n",
      " |-- PUviol_rt: float (nullable = true)\n",
      " |-- PUvolume_al: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jan_2017.printSchema()\n",
    "# Categorical Features\n",
    "# RateCodeID\n",
    "# store_and_fwd_flag\n",
    "# PULocationID\n",
    "# DOLocationID\n",
    "# LocationID (1 to 256)\n",
    "# PUBorough (comes from taxi+_lookup_zone)\n",
    "# PUZone (Name for Location ID)\n",
    "# PUServiceZone (Categorical)\n",
    "# PUNeighbor (Demographics Neighborhood)\n",
    "# PUDay (1-365)\n",
    "# PU_DOW (Day of week)\n",
    "# PUEvents\n",
    "# PUConditions\n",
    "# PUPeriod (AM or PM)\n",
    "#jan_2017.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUZone\", outputCol=\"PUZoneIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol='PUZoneIndex', outputCol=\"PUZoneVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUZoneIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol='RateCodeID', outputCol=\"RateCodeIDVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('RateCodeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OneHotEncoder(inputCol='store_and_fwd_flag', outputCol=\"store_and_fwd_flagVect\")\n",
    "# jan_2017 = encoder.transform(jan_2017).drop('store_and_fwd_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol='PLocationID', outputCol=\"PLocationIDVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jan_2017 = one_hot(jan_2017, 'DOLocationID')\n",
    "# print('done encoding DOLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jan_2017 = one_hot(jan_2017, 'LocationID')\n",
    "# print('done encoding LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUBorough\", outputCol=\"PUBoroughIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "encoder = OneHotEncoder(inputCol='PUBoroughIndex', outputCol=\"PUBoroughVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUBoroughIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUServiceZone\", outputCol=\"PUServiceZoneIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "encoder = OneHotEncoder(inputCol='PUServiceZoneIndex', outputCol=\"PUServiceZoneVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUServiceZoneIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexer = StringIndexer(inputCol=\"PUNeighbor\", outputCol=\"PUNeighborIndex\")\n",
    "# jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "# encoder = OneHotEncoder(inputCol='PUNeighborIndex', outputCol=\"PUNeighborVect\")\n",
    "# jan_2017 = encoder.transform(jan_2017).drop('PUNeighborIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol='PU_DOW', outputCol=\"PU_DOWVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PU_DOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUEvents\", outputCol=\"PUEventsIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol='PUEventsIndex', outputCol=\"PUEventsVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUEventsIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUConditions\", outputCol=\"PUConditionsIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol='PUConditionsIndex', outputCol=\"PUConditionsVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUConditionsIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"PUperiod\", outputCol=\"PUPeriodIndex\")\n",
    "jan_2017 = indexer.fit(jan_2017).transform(jan_2017)\n",
    "encoder = OneHotEncoder(inputCol='PUPeriodIndex', outputCol=\"PUPeriodVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUPeriodIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol='PUDay', outputCol=\"PUDayVect\")\n",
    "jan_2017 = encoder.transform(jan_2017).drop('PUDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: short (nullable = true)\n",
      " |-- passenger_count: short (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUServiceZone: string (nullable = true)\n",
      " |-- PUneighbor: string (nullable = true)\n",
      " |-- uniqueIdColumn: long (nullable = false)\n",
      " |-- AirportPU: integer (nullable = false)\n",
      " |-- PUHour: integer (nullable = true)\n",
      " |-- PUMinute: integer (nullable = true)\n",
      " |-- MorningRushHour: integer (nullable = false)\n",
      " |-- EveningRushHour: integer (nullable = false)\n",
      " |-- Weekend: integer (nullable = false)\n",
      " |-- WorkingHour: integer (nullable = false)\n",
      " |-- DLocationID: integer (nullable = true)\n",
      " |-- PUtemp: float (nullable = true)\n",
      " |-- PUwindchill: float (nullable = true)\n",
      " |-- PUdewpoint: float (nullable = true)\n",
      " |-- PUpressure: float (nullable = true)\n",
      " |-- PUvisibility: float (nullable = true)\n",
      " |-- PUwindDir: string (nullable = true)\n",
      " |-- PUgustSpeed: float (nullable = true)\n",
      " |-- PUPrecip: float (nullable = true)\n",
      " |-- PUEvents: string (nullable = true)\n",
      " |-- PUConditions: string (nullable = true)\n",
      " |-- PUperiod: string (nullable = true)\n",
      " |-- PUalone_hhld: float (nullable = true)\n",
      " |-- PUbachelor_higher: float (nullable = true)\n",
      " |-- PUbornstate: float (nullable = true)\n",
      " |-- PUcarfree: float (nullable = true)\n",
      " |-- PUcommutetime: float (nullable = true)\n",
      " |-- PUdisabled: float (nullable = true)\n",
      " |-- PUdisconnected: float (nullable = true)\n",
      " |-- PUforeign: float (nullable = true)\n",
      " |-- PUgross_rent_adj: float (nullable = true)\n",
      " |-- PUhhu18: float (nullable = true)\n",
      " |-- PUhomeownership: float (nullable = true)\n",
      " |-- PUhousing_units: float (nullable = true)\n",
      " |-- PUincome_diversity_ratio: float (nullable = true)\n",
      " |-- PUlaborforcerate: float (nullable = true)\n",
      " |-- PUmedhhincome_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_own_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_rent_adj: float (nullable = true)\n",
      " |-- PUnohsdiploma: float (nullable = true)\n",
      " |-- PUpark_share: float (nullable = true)\n",
      " |-- PUpasian: float (nullable = true)\n",
      " |-- PUpblack: float (nullable = true)\n",
      " |-- PUphisp: float (nullable = true)\n",
      " |-- PUpop65: float (nullable = true)\n",
      " |-- PUpopulation: float (nullable = true)\n",
      " |-- PUpopulation_density: float (nullable = true)\n",
      " |-- PUpov65older: float (nullable = true)\n",
      " |-- PUpovrate: float (nullable = true)\n",
      " |-- PUpovunder18: float (nullable = true)\n",
      " |-- PUprof_pct_ela: float (nullable = true)\n",
      " |-- PUprof_pct_math: float (nullable = true)\n",
      " |-- PUprop_rt: float (nullable = true)\n",
      " |-- PUpwhite: float (nullable = true)\n",
      " |-- PUrdindex: float (nullable = true)\n",
      " |-- PUrent_pct_nycha: float (nullable = true)\n",
      " |-- PUrentvacrate: float (nullable = true)\n",
      " |-- PUserious_viol_rate: float (nullable = true)\n",
      " |-- PUsevcrowd: float (nullable = true)\n",
      " |-- PUsubway_share: float (nullable = true)\n",
      " |-- PUtot_rt: float (nullable = true)\n",
      " |-- PUtotal_viol_rate: float (nullable = true)\n",
      " |-- PUunemprate: float (nullable = true)\n",
      " |-- PUviol_rt: float (nullable = true)\n",
      " |-- PUvolume_al: float (nullable = true)\n",
      " |-- PUZoneVect: vector (nullable = true)\n",
      " |-- RateCodeIDVect: vector (nullable = true)\n",
      " |-- PLocationIDVect: vector (nullable = true)\n",
      " |-- PUBoroughVect: vector (nullable = true)\n",
      " |-- PUServiceZoneVect: vector (nullable = true)\n",
      " |-- PU_DOWVect: vector (nullable = true)\n",
      " |-- PUEventsVect: vector (nullable = true)\n",
      " |-- PUConditionsVect: vector (nullable = true)\n",
      " |-- PUPeriodVect: vector (nullable = true)\n",
      " |-- PUDayVect: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jan_2017.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_points(X, labels):\n",
    "#         plt.scatter(X[:,0],X[:,1])\n",
    "#         for i, txt in enumerate(labels):\n",
    "#                 plt.annotate(txt, (X[i,0],X[i,1]))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PCA!!!\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Select Labels\n",
    "# labels = jan_2017['PUZone']\n",
    "\n",
    "# # Standardize everything\n",
    "# standsac = StandardScaler()\n",
    "# jan_2017_std= standsac.fit_transform(jan_2017)\n",
    "\n",
    "# # Compute PCA with 2 Principal Components\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(jan_2017_std)\n",
    "# jan_2017_PCA = pca.transform(jan_2017_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampe = jan_2017.sample(False,0.0001,0).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sampe.schema.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampe.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #colum = sampe.schema.names\n",
    "# sampe = sampe.withColumn(\"PLocationID\", sampe.PULocationID.cast(IntegerType())).drop(\"PULocationID\")\n",
    "# sampe = sampe.withColumn(\"DLocationID\", sampe.DOLocationID.cast(IntegerType())).drop(\"DOLocationID\")\n",
    "# #sampe.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampe = sampe.withColumn(\"DLocationID\", sampe.DOLocationID.cast(IntegerType())).drop(\"DOLocationID\")\n",
    "# #sampe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any string columns except 'PUZone'\n",
    "str_names = ['PUZone','tpep_pickup_datetime','tpep_dropoff_datetime','store_and_fwd_flag','PULocationID',\\\n",
    "             'LocationID','PUBorough','PUServiceZone','PUneighbor','PUTime','PUTemptime',\\\n",
    "             'PUwindDir','PUEvents','PUConditions','PUperiod','neighborhood','PUDate', 'PUTempdate']\n",
    "for col in str_names:\n",
    "    sampe=sampe.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum = sampe.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum.remove('DLocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=colum,\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "# indexerModel = indexer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create new column \"indexed\" with categorical values transformed to indices\n",
    "# putput = indexerModel.transform(train)\n",
    "# indexedData.show()\n",
    "\n",
    "\n",
    "output = assembler.transform(sampe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: short (nullable = true)\n",
      " |-- passenger_count: short (nullable = true)\n",
      " |-- uniqueIdColumn: long (nullable = false)\n",
      " |-- AirportPU: integer (nullable = false)\n",
      " |-- PUHour: integer (nullable = true)\n",
      " |-- PUMinute: integer (nullable = true)\n",
      " |-- MorningRushHour: integer (nullable = false)\n",
      " |-- EveningRushHour: integer (nullable = false)\n",
      " |-- Weekend: integer (nullable = false)\n",
      " |-- WorkingHour: integer (nullable = false)\n",
      " |-- DLocationID: integer (nullable = true)\n",
      " |-- PUtemp: float (nullable = true)\n",
      " |-- PUwindchill: float (nullable = true)\n",
      " |-- PUdewpoint: float (nullable = true)\n",
      " |-- PUpressure: float (nullable = true)\n",
      " |-- PUvisibility: float (nullable = true)\n",
      " |-- PUgustSpeed: float (nullable = true)\n",
      " |-- PUPrecip: float (nullable = true)\n",
      " |-- PUalone_hhld: float (nullable = true)\n",
      " |-- PUbachelor_higher: float (nullable = true)\n",
      " |-- PUbornstate: float (nullable = true)\n",
      " |-- PUcarfree: float (nullable = true)\n",
      " |-- PUcommutetime: float (nullable = true)\n",
      " |-- PUdisabled: float (nullable = true)\n",
      " |-- PUdisconnected: float (nullable = true)\n",
      " |-- PUforeign: float (nullable = true)\n",
      " |-- PUgross_rent_adj: float (nullable = true)\n",
      " |-- PUhhu18: float (nullable = true)\n",
      " |-- PUhomeownership: float (nullable = true)\n",
      " |-- PUhousing_units: float (nullable = true)\n",
      " |-- PUincome_diversity_ratio: float (nullable = true)\n",
      " |-- PUlaborforcerate: float (nullable = true)\n",
      " |-- PUmedhhincome_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_own_adj: float (nullable = true)\n",
      " |-- PUmedhhincome_rent_adj: float (nullable = true)\n",
      " |-- PUnohsdiploma: float (nullable = true)\n",
      " |-- PUpark_share: float (nullable = true)\n",
      " |-- PUpasian: float (nullable = true)\n",
      " |-- PUpblack: float (nullable = true)\n",
      " |-- PUphisp: float (nullable = true)\n",
      " |-- PUpop65: float (nullable = true)\n",
      " |-- PUpopulation: float (nullable = true)\n",
      " |-- PUpopulation_density: float (nullable = true)\n",
      " |-- PUpov65older: float (nullable = true)\n",
      " |-- PUpovrate: float (nullable = true)\n",
      " |-- PUpovunder18: float (nullable = true)\n",
      " |-- PUprof_pct_ela: float (nullable = true)\n",
      " |-- PUprof_pct_math: float (nullable = true)\n",
      " |-- PUprop_rt: float (nullable = true)\n",
      " |-- PUpwhite: float (nullable = true)\n",
      " |-- PUrdindex: float (nullable = true)\n",
      " |-- PUrent_pct_nycha: float (nullable = true)\n",
      " |-- PUrentvacrate: float (nullable = true)\n",
      " |-- PUserious_viol_rate: float (nullable = true)\n",
      " |-- PUsevcrowd: float (nullable = true)\n",
      " |-- PUsubway_share: float (nullable = true)\n",
      " |-- PUtot_rt: float (nullable = true)\n",
      " |-- PUtotal_viol_rate: float (nullable = true)\n",
      " |-- PUunemprate: float (nullable = true)\n",
      " |-- PUviol_rt: float (nullable = true)\n",
      " |-- PUvolume_al: float (nullable = true)\n",
      " |-- PUZoneVect: vector (nullable = true)\n",
      " |-- RateCodeIDVect: vector (nullable = true)\n",
      " |-- PLocationIDVect: vector (nullable = true)\n",
      " |-- PUBoroughVect: vector (nullable = true)\n",
      " |-- PUServiceZoneVect: vector (nullable = true)\n",
      " |-- PU_DOWVect: vector (nullable = true)\n",
      " |-- PUEventsVect: vector (nullable = true)\n",
      " |-- PUConditionsVect: vector (nullable = true)\n",
      " |-- PUPeriodVect: vector (nullable = true)\n",
      " |-- PUDayVect: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = output.select([\"DLocationID\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"label\", train[\"DLocationID\"]).drop(\"DLocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.select('features').limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.select([\"label\",\"features\"]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o985.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 59.0 failed 1 times, most recent failure: Lost task 5.0 in stage 59.0 (TID 3677, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply10_24$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)\n\t... 58 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:334)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:387)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:48)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:99)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply10_24$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-680ef3bd32f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o985.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 59.0 failed 1 times, most recent failure: Lost task 5.0 in stage 59.0 (TID 3677, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply10_24$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)\n\t... 58 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:334)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:387)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:48)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:99)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply10_24$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)\n\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "model = pca.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(train).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = train.select('features').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = RowMatrix(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc = mat.computePrincipalComponents(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected = mat.multiply(pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = train.randomSplit([0.6, 0.4], 1234)\n",
    "# train = splits[0]\n",
    "# test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# layers = [4, 5, 4, 3]\n",
    "\n",
    "# # create the trainer and set its parameters\n",
    "# trainer = MultilayerPerceptronClassifier(maxIter=2, layers=layers, blockSize=128, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# model = trainer.fit(train)\n",
    "\n",
    "# # compute accuracy on the test set\n",
    "# result = model.transform(test)\n",
    "# predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "# evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "# print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
